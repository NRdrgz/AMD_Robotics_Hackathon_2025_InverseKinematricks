{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQUk3Y0WwYZ4"
   },
   "source": [
    "# Train Models Using LeRobot on MI300x\n",
    "\n",
    "This guide walks you through setting up environment for training imitation learning policies using LeRobot library on a DigitalOcean (DO) instance equipped with AMD MI300x GPUs and ROCm.\n",
    "\n",
    "## ‚öôÔ∏è Requirements\n",
    "\n",
    "- A Hugging Face dataset repo ID containing your training data (`--dataset.repo_id=${HF_USER}/${DATASET_NAME}`).\n",
    "  If you don‚Äôt have an access token yet, you can sign up for Hugging Face [here](https://huggingface.co/join). After signing up, create an access token by visiting [here](https://huggingface.co/settings/tokens).\n",
    "- A wandb account to enable training visualization and upload your training evidence to our github.\n",
    "  You can sign up for Wandb [here](https://wandb.ai/signup) and visit [here](https://wandb.ai/authorize) to create a token.\n",
    "- Access to DO instance AMD Mi300x GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOJyX0CnwA5m"
   },
   "source": [
    "## Verify ROCm and GPU availability\n",
    "\n",
    "This cell uses `pytorch` to check AMD GPU Info. The expected ouput is\n",
    "\n",
    "```\n",
    "CUDA compatible device availability: True\n",
    "device name [0]: AMD Instinct MI300X VF\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA compatible device availability: True\n",
      "device name [0]: AMD Instinct MI300X VF\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA compatible device availability:\", torch.cuda.is_available())\n",
    "print(f\"device name [0]:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOJyX0CnwA5m"
   },
   "source": [
    "## Install FFmpeg 7.x\n",
    "\n",
    "This cell uses `apt` to install ffmpeg 7.x for LeRobot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlKjL1X5t_zM",
    "outputId": "3b375aa1-19ed-4811-ff76-0afd5b762992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository: 'Types: deb\n",
      "URIs: https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu/\n",
      "Suites: noble\n",
      "Components: main\n",
      "'\n",
      "Description:\n",
      "unofficial build for FFmpeg 7 for Ubuntu 22.04 | 24.04, backport from Debian's deb.multimedia.org repository\n",
      "\n",
      "If the packages here are helpful, you may buy me a coffee:\n",
      "\n",
      "¬†¬†¬†¬†¬†¬†¬†¬†¬†https://ko-fi.com/ubuntuhandbook1\n",
      "More info: https://launchpad.net/~ubuntuhandbook1/+archive/ubuntu/ffmpeg7\n",
      "Adding repository.\n",
      "Found existing deb entry in /etc/apt/sources.list.d/ubuntuhandbook1-ubuntu-ffmpeg7-noble.sources\n",
      "Hit:1 https://repo.radeon.com/amdgpu/30.10/ubuntu jammy InRelease\n",
      "Hit:2 https://repo.radeon.com/rocm/apt/7.0 jammy InRelease                     \n",
      "Hit:3 https://repo.radeon.com/graphics/7.0/ubuntu jammy InRelease              \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu noble InRelease                         \n",
      "Get:5 http://archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]        \n",
      "Get:6 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]      \n",
      "Get:7 http://archive.ubuntu.com/ubuntu noble-backports InRelease [126 kB]      \n",
      "Get:8 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [2118 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Components [235 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [1944 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Components [521 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu noble-updates/restricted amd64 Components [159 B]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu noble-updates/multiverse amd64 Components [887 B]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu noble-backports/main amd64 Components [7868 B]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu noble-backports/universe amd64 Components [11.9 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu noble-backports/restricted amd64 Components [160 B]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu noble-backports/multiverse amd64 Components [162 B]\n",
      "Hit:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu noble InRelease  \n",
      "Get:19 http://security.ubuntu.com/ubuntu noble-security/main amd64 Components [25.4 kB]\n",
      "Hit:20 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble InRelease\n",
      "Get:21 http://security.ubuntu.com/ubuntu noble-security/universe amd64 Components [90.3 kB]\n",
      "Get:22 http://security.ubuntu.com/ubuntu noble-security/restricted amd64 Components [155 B]\n",
      "Get:23 http://security.ubuntu.com/ubuntu noble-security/multiverse amd64 Components [158 B]\n",
      "Fetched 5333 kB in 1s (6885 kB/s)         \n",
      "Reading package lists... Done\n",
      "Hit:1 https://repo.radeon.com/amdgpu/30.10/ubuntu jammy InRelease\n",
      "Hit:2 https://repo.radeon.com/rocm/apt/7.0 jammy InRelease                     \u001b[0m\n",
      "Hit:3 https://repo.radeon.com/graphics/7.0/ubuntu jammy InRelease              \u001b[0m\n",
      "Hit:4 http://security.ubuntu.com/ubuntu noble-security InRelease               \u001b[0m\u001b[33m\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu noble InRelease                         \u001b[0m\u001b[33m\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu noble-updates InRelease\n",
      "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu noble InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble InRelease\n",
      "Hit:9 http://archive.ubuntu.com/ubuntu noble-backports InRelease\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "93 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:7.1.1-0build1~ubuntu2404).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 93 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!add-apt-repository ppa:ubuntuhandbook1/ffmpeg7 -y # install PPA which contains ffmpeg 7.x\n",
    "!apt update && apt install ffmpeg -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxCc3CARwUjN"
   },
   "source": [
    "## Install LeRobot v0.4.1\n",
    "\n",
    "This cell clones the `lerobot` repository from Hugging Face, and installs the package in editable mode. Extra Features: To install additional dependencies for training SmolVLA or Pi models, refer to the [LeRobot offical page](https://huggingface.co/docs/lerobot/index).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgLu7QT5tUik",
    "outputId": "e46913b8-1977-48a5-a851-d8c69602419a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'lerobot' already exists and is not an empty directory.\n",
      "warning: refname 'v0.4.1' is ambiguous.\n",
      "fatal: a branch named 'v0.4.1' already exists\n",
      "Obtaining file:///workspace/lerobot\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets<4.2.0,>=4.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.1.1)\n",
      "Requirement already satisfied: diffusers<0.36.0,>=0.27.2 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.35.2)\n",
      "Requirement already satisfied: huggingface-hub<0.36.0,>=0.34.2 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.35.3)\n",
      "Requirement already satisfied: accelerate<2.0.0,>=1.10.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.12.0)\n",
      "Requirement already satisfied: setuptools<81.0.0,>=71.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (80.9.0)\n",
      "Requirement already satisfied: cmake<4.2.0,>=3.29.0.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.1.3)\n",
      "Requirement already satisfied: einops<0.9.0,>=0.8.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.8.1)\n",
      "Requirement already satisfied: opencv-python-headless<4.13.0,>=4.9.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.12.0.88)\n",
      "Requirement already satisfied: av<16.0.0,>=15.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (15.1.0)\n",
      "Requirement already satisfied: jsonlines<5.0.0,>=4.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.0.0)\n",
      "Requirement already satisfied: packaging<26.0,>=24.2 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (25.0)\n",
      "Requirement already satisfied: pynput<1.9.0,>=1.7.7 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.8.1)\n",
      "Requirement already satisfied: pyserial<4.0,>=3.5 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (3.5)\n",
      "Requirement already satisfied: wandb<0.22.0,>=0.20.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.21.4)\n",
      "Requirement already satisfied: torch<2.8.0,>=2.2.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (2.7.1+rocm7.0.0.lw.git698b58a9)\n",
      "Requirement already satisfied: torchcodec<0.6.0,>=0.2.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.5)\n",
      "Requirement already satisfied: torchvision<0.23.0,>=0.21.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.22.1+rocm7.0.0.git59a3e1f9)\n",
      "Requirement already satisfied: draccus==0.10.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.10.0)\n",
      "Requirement already satisfied: gymnasium<2.0.0,>=1.1.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.2.2)\n",
      "Requirement already satisfied: rerun-sdk<0.27.0,>=0.24.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.26.2)\n",
      "Requirement already satisfied: deepdiff<9.0.0,>=7.0.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (8.6.1)\n",
      "Requirement already satisfied: imageio<3.0.0,>=2.34.0 in /opt/venv/lib/python3.12/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.1) (2.37.2)\n",
      "Requirement already satisfied: termcolor<4.0.0,>=2.4.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (3.2.0)\n",
      "Collecting num2words<0.6.0,>=0.5.14 (from lerobot==0.4.1)\n",
      "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: safetensors<1.0.0,>=0.4.3 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.7.0)\n",
      "Requirement already satisfied: mergedeep~=1.3 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (1.3.4)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (6.0.2)\n",
      "Requirement already satisfied: pyyaml-include~=1.4 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (1.4.1)\n",
      "Requirement already satisfied: toml~=0.10 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (0.10.2)\n",
      "Requirement already satisfied: typing-inspect~=0.9.0 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/venv/lib/python3.12/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1) (2.2.6)\n",
      "Requirement already satisfied: psutil in /opt/venv/lib/python3.12/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1) (7.1.3)\n",
      "Requirement already satisfied: filelock in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.9.0)\n",
      "Requirement already satisfied: orderly-set<6,>=5.4.1 in /opt/venv/lib/python3.12/site-packages (from deepdiff<9.0.0,>=7.0.1->lerobot==0.4.1) (5.5.0)\n",
      "Requirement already satisfied: importlib_metadata in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (8.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (2025.11.3)\n",
      "Requirement already satisfied: Pillow in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (11.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.13.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (0.0.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub<0.36.0,>=0.34.2->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (1.2.0)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.3.4)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.1.9)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /opt/venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (3.0.52)\n",
      "Requirement already satisfied: imageio-ffmpeg in /opt/venv/lib/python3.12/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.1) (0.6.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/venv/lib/python3.12/site-packages (from jsonlines<5.0.0,>=4.0.0->lerobot==0.4.1) (25.4.0)\n",
      "Collecting docopt>=0.6.2 (from num2words<0.6.0,>=0.5.14->lerobot==0.4.1)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /opt/venv/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.2.14)\n",
      "Requirement already satisfied: six in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (1.17.0)\n",
      "Requirement already satisfied: evdev>=1.3 in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (1.9.2)\n",
      "Requirement already satisfied: python-xlib>=0.17 in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (0.33)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.1.6)\n",
      "Requirement already satisfied: pytorch-triton-rocm==3.3.1+rocm7.0.0.git9c7bc0a3 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.3.1+rocm7.0.0.git9c7bc0a3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/venv/lib/python3.12/site-packages (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot==0.4.1) (1.1.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (4.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (6.33.2)\n",
      "Requirement already satisfied: pydantic<3 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.12.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.47.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.22.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (5.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<2.8.0,>=2.2.1->lerobot==0.4.1) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/venv/lib/python3.12/site-packages (from importlib_metadata->diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/venv/lib/python3.12/site-packages (from jinja2->torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.0.2)\n",
      "Collecting transformers<5.0.0,>=4.53.0 (from lerobot==0.4.1)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.53.0->lerobot==0.4.1)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.2)\n",
      "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lerobot, docopt\n",
      "  Building editable for lerobot (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lerobot: filename=lerobot-0.4.1-0.editable-py3-none-any.whl size=15631 sha256=e1a74674eac9e4260e4cde92c351ab3d22bf428a666293761a6a49ff59fc826d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mv247b40/wheels/05/0a/0d/80a4c08845345c44fe1e5f70929884983b90d85f46a77f7601\n",
      "  Building wheel for docopt (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13783 sha256=dc828783f5baf484f0c1829391fd9799c7d030ffd1269ca6eb63aaedf7ea4d0f\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "Successfully built lerobot docopt\n",
      "Installing collected packages: docopt, num2words, tokenizers, transformers, lerobot\n",
      "\u001b[2K  Attempting uninstall: lerobot‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/5\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: lerobot 0.4.1m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/5\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling lerobot-0.4.1:m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/5\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled lerobot-0.4.1\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/5\u001b[0m [lerobot]]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5/5\u001b[0m [lerobot]\u001b[0m [lerobot]\n",
      "\u001b[1A\u001b[2KSuccessfully installed docopt-0.6.2 lerobot-0.4.1 num2words-0.5.14 tokenizers-0.22.1 transformers-4.57.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/lerobot.git\n",
    "!cd lerobot && git checkout -b v0.4.1 v0.4.1 # let‚Äôs synchronize using this version\n",
    "!cd lerobot && pip install -e .[smolvla]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8Sn2wG4wldo"
   },
   "source": [
    "## Weights & Biases login\n",
    "\n",
    "This cell install and log into Weights & Biases (wandb) to enable experiment tracking and logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PolVM_movEvp",
    "outputId": "1769c6bd-8644-4b65-84c7-4f020b234c92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/venv/lib/python3.12/site-packages (0.21.4)\n",
      "Requirement already satisfied: click>=8.0.1 in /opt/venv/lib/python3.12/site-packages (from wandb) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in /opt/venv/lib/python3.12/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /opt/venv/lib/python3.12/site-packages (from wandb) (4.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/venv/lib/python3.12/site-packages (from wandb) (6.33.2)\n",
      "Requirement already satisfied: pydantic<3 in /opt/venv/lib/python3.12/site-packages (from wandb) (2.12.5)\n",
      "Requirement already satisfied: pyyaml in /opt/venv/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb) (2.47.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /opt/venv/lib/python3.12/site-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wandb\n",
    "import wandb\n",
    "\n",
    "wandb.login(key=\"<WANDB_KEY>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login into Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PolVM_movEvp",
    "outputId": "1769c6bd-8644-4b65-84c7-4f020b234c92"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"<HF_KEY>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login, list_repo_tree\n",
    "from datasets import load_dataset\n",
    "\n",
    "repo_id = \"giacomoran/hackathon_amd_mission2_black_sort\"\n",
    "\n",
    "# Method 1: List episode files from the repo structure\n",
    "print(\"üìÅ Episode files in dataset:\")\n",
    "api = HfApi()\n",
    "for item in list_repo_tree(repo_id=repo_id, repo_type=\"dataset\", recursive=True):\n",
    "    if \"episode\" in item.path.lower() or item.path.endswith(\".parquet\"):\n",
    "        print(f\"  {item.path}\")\n",
    "\n",
    "# Method 2: Load dataset and check episode info\n",
    "print(\"\\nüìä Loading dataset metadata...\")\n",
    "dataset = load_dataset(repo_id, split=\"train\")\n",
    "\n",
    "# Check for episode column\n",
    "if \"episode_index\" in dataset.column_names:\n",
    "    episodes = sorted(set(dataset[\"episode_index\"]))\n",
    "    print(f\"\\n‚úÖ Found {len(episodes)} episodes:\")\n",
    "    for ep in episodes:\n",
    "        ep_data = dataset.filter(lambda x: x[\"episode_index\"] == ep)\n",
    "        print(f\"  Episode {ep}: {len(ep_data)} frames\")\n",
    "else:\n",
    "    print(f\"\\nDataset columns: {dataset.column_names}\")\n",
    "    print(f\"Total rows: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkzTo4mNwxaC"
   },
   "source": [
    "## Start Training Models with LeRobot\n",
    "\n",
    "This cell uses the lerobot-train CLI from the lerobot library to train a robot control policy.\n",
    "\n",
    "Make sure to adjust the following arguments to your setup:\n",
    "\n",
    "1. `--dataset.repo_id=YOUR_HF_USERNAME/YOUR_DATASET`:  \n",
    "   Replace this with the Hugging Face Hub repo ID where your dataset is stored, e.g., `lerobot/svla_so100_pickplace`.\n",
    "\n",
    "2. `--policy.type=act`:  \n",
    "   Specifies the policy configuration to use. `act` refers to [configuration_act.py](../lerobot/common/policies/act/configuration_act.py), which will automatically adapt to your dataset‚Äôs setup (e.g., number of motors and cameras).\n",
    "\n",
    "3. `--output_dir=outputs/train/...`:  \n",
    "   Directory where training logs and model checkpoints will be saved.\n",
    "\n",
    "4. `--job_name=...`:  \n",
    "   A name for this training job, used for logging and Weights & Biases.The name typically includes the model type (e.g., act, smolvla), the dataset name, and additional descriptive tags.\n",
    "\n",
    "5. `--policy.device=cuda`:  \n",
    "   Use `cuda` if training on an AMD or NVIDIA GPU.\n",
    "\n",
    "6. `--wandb.enable=true`:  \n",
    "   Enables Weights & Biases for visualizing training progress. You must be logged in via `wandb login` before running this.\n",
    "\n",
    "7. `--policy.push_to_hub=`:\n",
    "\n",
    "   Enables automatic uploading of the trained policy to the Hugging Face Hub. You must specify `--policy.repo_id` (e.g., ${HF_USER}/{REPO_NAME}) if it is True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf outputs/train/hackathon_amd_mission2_black_sort_smolvla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ufss6US6xbpi",
    "outputId": "cfd494c7-8689-419d-bb17-7f02a67b26d6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-12-13 13:32:02 ot_train.py:163 {'batch_size': 64,\n",
      " 'checkpoint_path': None,\n",
      " 'dataset': {'episodes': None,\n",
      "             'image_transforms': {'enable': False,\n",
      "                                  'max_num_transforms': 3,\n",
      "                                  'random_order': False,\n",
      "                                  'tfs': {'affine': {'kwargs': {'degrees': [-5.0,\n",
      "                                                                            5.0],\n",
      "                                                                'translate': [0.05,\n",
      "                                                                              0.05]},\n",
      "                                                     'type': 'RandomAffine',\n",
      "                                                     'weight': 1.0},\n",
      "                                          'brightness': {'kwargs': {'brightness': [0.8,\n",
      "                                                                                   1.2]},\n",
      "                                                         'type': 'ColorJitter',\n",
      "                                                         'weight': 1.0},\n",
      "                                          'contrast': {'kwargs': {'contrast': [0.8,\n",
      "                                                                               1.2]},\n",
      "                                                       'type': 'ColorJitter',\n",
      "                                                       'weight': 1.0},\n",
      "                                          'hue': {'kwargs': {'hue': [-0.05,\n",
      "                                                                     0.05]},\n",
      "                                                  'type': 'ColorJitter',\n",
      "                                                  'weight': 1.0},\n",
      "                                          'saturation': {'kwargs': {'saturation': [0.5,\n",
      "                                                                                   1.5]},\n",
      "                                                         'type': 'ColorJitter',\n",
      "                                                         'weight': 1.0},\n",
      "                                          'sharpness': {'kwargs': {'sharpness': [0.5,\n",
      "                                                                                 1.5]},\n",
      "                                                        'type': 'SharpnessJitter',\n",
      "                                                        'weight': 1.0}}},\n",
      "             'repo_id': 'giacomoran/hackathon_amd_mission2_black_sort',\n",
      "             'revision': None,\n",
      "             'root': None,\n",
      "             'streaming': False,\n",
      "             'use_imagenet_stats': True,\n",
      "             'video_backend': 'torchcodec'},\n",
      " 'env': None,\n",
      " 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},\n",
      " 'eval_freq': 20000,\n",
      " 'job_name': 'hackathon_amd_mission2_black_sort_smolvla',\n",
      " 'log_freq': 200,\n",
      " 'num_workers': 4,\n",
      " 'optimizer': {'betas': [0.9, 0.95],\n",
      "               'eps': 1e-08,\n",
      "               'grad_clip_norm': 10.0,\n",
      "               'lr': 0.0001,\n",
      "               'type': 'adamw',\n",
      "               'weight_decay': 1e-10},\n",
      " 'output_dir': 'outputs/train/hackathon_amd_mission2_black_sort_smolvla',\n",
      " 'policy': {'adapt_to_pi_aloha': False,\n",
      "            'add_image_special_tokens': False,\n",
      "            'attention_mode': 'cross_attn',\n",
      "            'chunk_size': 30,\n",
      "            'device': 'cuda',\n",
      "            'empty_cameras': 1,\n",
      "            'expert_width_multiplier': 0.75,\n",
      "            'freeze_vision_encoder': True,\n",
      "            'input_features': {'observation.images.camera1': {'shape': [3,\n",
      "                                                                        256,\n",
      "                                                                        256],\n",
      "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
      "                               'observation.images.camera2': {'shape': [3,\n",
      "                                                                        256,\n",
      "                                                                        256],\n",
      "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
      "                               'observation.images.camera3': {'shape': [3,\n",
      "                                                                        256,\n",
      "                                                                        256],\n",
      "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
      "                               'observation.state': {'shape': [6],\n",
      "                                                     'type': <FeatureType.STATE: 'STATE'>}},\n",
      "            'license': None,\n",
      "            'load_vlm_weights': True,\n",
      "            'max_action_dim': 32,\n",
      "            'max_period': 4.0,\n",
      "            'max_state_dim': 32,\n",
      "            'min_period': 0.004,\n",
      "            'n_action_steps': 30,\n",
      "            'n_obs_steps': 1,\n",
      "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
      "                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
      "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
      "            'num_expert_layers': 0,\n",
      "            'num_steps': 10,\n",
      "            'num_vlm_layers': 16,\n",
      "            'optimizer_betas': [0.9, 0.95],\n",
      "            'optimizer_eps': 1e-08,\n",
      "            'optimizer_grad_clip_norm': 10.0,\n",
      "            'optimizer_lr': 0.0001,\n",
      "            'optimizer_weight_decay': 1e-10,\n",
      "            'output_features': {'action': {'shape': [6],\n",
      "                                           'type': <FeatureType.ACTION: 'ACTION'>}},\n",
      "            'pad_language_to': 'max_length',\n",
      "            'prefix_length': 0,\n",
      "            'pretrained_path': 'lerobot/smolvla_base',\n",
      "            'private': None,\n",
      "            'push_to_hub': True,\n",
      "            'repo_id': 'giacomoran/hackathon_amd_mission2_black_sort_smolvla',\n",
      "            'resize_imgs_with_padding': [512, 512],\n",
      "            'scheduler_decay_lr': 2.5e-06,\n",
      "            'scheduler_decay_steps': 30000,\n",
      "            'scheduler_warmup_steps': 1000,\n",
      "            'self_attn_every_n_layers': 2,\n",
      "            'tags': None,\n",
      "            'tokenizer_max_length': 48,\n",
      "            'train_expert_only': True,\n",
      "            'train_state_proj': True,\n",
      "            'type': 'smolvla',\n",
      "            'use_amp': False,\n",
      "            'use_cache': True,\n",
      "            'use_delta_joint_actions_aloha': False,\n",
      "            'vlm_model_name': 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'},\n",
      " 'rename_map': {'observation.images.top': 'observation.images.camera1',\n",
      "                'observation.images.wrist': 'observation.images.camera2'},\n",
      " 'resume': False,\n",
      " 'save_checkpoint': True,\n",
      " 'save_freq': 5000,\n",
      " 'scheduler': {'decay_lr': 2.5e-06,\n",
      "               'num_decay_steps': 30000,\n",
      "               'num_warmup_steps': 1000,\n",
      "               'peak_lr': 0.0001,\n",
      "               'type': 'cosine_decay_with_warmup'},\n",
      " 'seed': 1000,\n",
      " 'steps': 20000,\n",
      " 'use_policy_training_preset': True,\n",
      " 'wandb': {'disable_artifact': False,\n",
      "           'enable': True,\n",
      "           'entity': None,\n",
      "           'mode': None,\n",
      "           'notes': None,\n",
      "           'project': 'lerobot',\n",
      "           'run_id': None}}\n",
      "/opt/venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/opt/venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "INFO 2025-12-13 13:32:04 db_utils.py:102 \u001b[1m\u001b[34mLogs will be synced with wandb.\u001b[0m\n",
      "INFO 2025-12-13 13:32:04 db_utils.py:103 Track this run --> \u001b[1m\u001b[33mhttps://wandb.ai/fengxiao0730-/lerobot/runs/pvlxihar\u001b[0m\n",
      "INFO 2025-12-13 13:32:04 ot_train.py:183 Creating dataset\n",
      "INFO 2025-12-13 13:32:04 ot_train.py:202 Creating policy\n",
      "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Reducing the number of VLM layers to 16 ...\n",
      "INFO 2025-12-13 13:32:12 ot_train.py:247 Creating optimizer and scheduler\n",
      "INFO 2025-12-13 13:32:12 hedulers.py:105 Auto-scaling LR scheduler: num_training_steps (20000) < num_decay_steps (30000). Scaling warmup: 1000 ‚Üí 666, decay: 30000 ‚Üí 20000 (scale factor: 0.667)\n",
      "INFO 2025-12-13 13:32:12 ot_train.py:259 \u001b[1m\u001b[33mOutput dir:\u001b[0m outputs/train/hackathon_amd_mission2_black_sort_smolvla\n",
      "INFO 2025-12-13 13:32:12 ot_train.py:262 cfg.steps=20000 (20K)\n",
      "INFO 2025-12-13 13:32:12 ot_train.py:263 dataset.num_frames=18940 (19K)\n",
      "INFO 2025-12-13 13:32:12 ot_train.py:264 dataset.num_episodes=60\n",
      "INFO 2025-12-13 13:32:12 ot_train.py:267 Effective batch size: 64 x 1 = 64\n",
      "INFO 2025-12-13 13:32:12 ot_train.py:268 num_learnable_params=99880992 (100M)\n",
      "INFO 2025-12-13 13:32:12 ot_train.py:269 num_total_params=450046176 (450M)\n",
      "INFO 2025-12-13 13:32:12 ot_train.py:324 Start offline training on a fixed dataset\n",
      "INFO 2025-12-13 13:34:08 ot_train.py:351 step:200 smpl:13K ep:41 epch:0.68 loss:0.093 grdn:0.588 lr:1.5e-05 updt_s:0.554 data_s:0.026\n",
      "WARNING 2025-12-13 13:34:08 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:34:08 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 13:35:50 ot_train.py:351 step:400 smpl:26K ep:81 epch:1.35 loss:0.055 grdn:0.512 lr:4.5e-05 updt_s:0.467 data_s:0.041\n",
      "WARNING 2025-12-13 13:35:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:35:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 13:37:33 ot_train.py:351 step:600 smpl:38K ep:122 epch:2.03 loss:0.049 grdn:0.510 lr:7.5e-05 updt_s:0.465 data_s:0.047\n",
      "WARNING 2025-12-13 13:37:33 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:37:33 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 13:39:09 ot_train.py:351 step:800 smpl:51K ep:162 epch:2.70 loss:0.048 grdn:0.506 lr:9.8e-05 updt_s:0.462 data_s:0.015\n",
      "WARNING 2025-12-13 13:39:09 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:39:09 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 13:40:51 ot_train.py:351 step:1K smpl:64K ep:203 epch:3.38 loss:0.043 grdn:0.450 lr:1.0e-04 updt_s:0.463 data_s:0.043\n",
      "WARNING 2025-12-13 13:40:51 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:40:51 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 13:42:33 ot_train.py:351 step:1K smpl:77K ep:243 epch:4.05 loss:0.039 grdn:0.431 lr:9.9e-05 updt_s:0.464 data_s:0.045\n",
      "WARNING 2025-12-13 13:42:33 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:42:33 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 13:44:08 ot_train.py:351 step:1K smpl:90K ep:284 epch:4.73 loss:0.036 grdn:0.398 lr:9.9e-05 updt_s:0.461 data_s:0.015\n",
      "WARNING 2025-12-13 13:44:08 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:44:08 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 13:45:50 ot_train.py:351 step:2K smpl:102K ep:324 epch:5.41 loss:0.034 grdn:0.389 lr:9.9e-05 updt_s:0.463 data_s:0.046\n",
      "WARNING 2025-12-13 13:45:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:45:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 13:47:33 ot_train.py:351 step:2K smpl:115K ep:365 epch:6.08 loss:0.032 grdn:0.366 lr:9.8e-05 updt_s:0.462 data_s:0.048\n",
      "WARNING 2025-12-13 13:47:33 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:47:33 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 13:49:09 ot_train.py:351 step:2K smpl:128K ep:405 epch:6.76 loss:0.029 grdn:0.354 lr:9.8e-05 updt_s:0.462 data_s:0.015\n",
      "WARNING 2025-12-13 13:49:09 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:49:09 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 13:50:51 ot_train.py:351 step:2K smpl:141K ep:446 epch:7.43 loss:0.028 grdn:0.343 lr:9.7e-05 updt_s:0.464 data_s:0.045\n",
      "WARNING 2025-12-13 13:50:51 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:50:51 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 13:52:32 ot_train.py:351 step:2K smpl:154K ep:487 epch:8.11 loss:0.027 grdn:0.342 lr:9.7e-05 updt_s:0.465 data_s:0.040\n",
      "WARNING 2025-12-13 13:52:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:52:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 13:54:09 ot_train.py:351 step:3K smpl:166K ep:527 epch:8.79 loss:0.025 grdn:0.326 lr:9.6e-05 updt_s:0.466 data_s:0.015\n",
      "WARNING 2025-12-13 13:54:09 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:54:09 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 13:55:52 ot_train.py:351 step:3K smpl:179K ep:568 epch:9.46 loss:0.024 grdn:0.315 lr:9.6e-05 updt_s:0.466 data_s:0.048\n",
      "WARNING 2025-12-13 13:55:52 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:55:52 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 13:57:35 ot_train.py:351 step:3K smpl:192K ep:608 epch:10.14 loss:0.023 grdn:0.319 lr:9.5e-05 updt_s:0.465 data_s:0.047\n",
      "WARNING 2025-12-13 13:57:35 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:57:35 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 13:59:10 ot_train.py:351 step:3K smpl:205K ep:649 epch:10.81 loss:0.022 grdn:0.305 lr:9.4e-05 updt_s:0.464 data_s:0.015\n",
      "WARNING 2025-12-13 13:59:10 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 13:59:10 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 14:00:52 ot_train.py:351 step:3K smpl:218K ep:689 epch:11.49 loss:0.021 grdn:0.302 lr:9.4e-05 updt_s:0.466 data_s:0.040\n",
      "WARNING 2025-12-13 14:00:52 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 14:00:52 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 14:02:35 ot_train.py:351 step:4K smpl:230K ep:730 epch:12.16 loss:0.021 grdn:0.286 lr:9.3e-05 updt_s:0.465 data_s:0.049\n",
      "WARNING 2025-12-13 14:02:35 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 14:02:35 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 14:04:11 ot_train.py:351 step:4K smpl:243K ep:770 epch:12.84 loss:0.019 grdn:0.282 lr:9.2e-05 updt_s:0.464 data_s:0.015\n",
      "WARNING 2025-12-13 14:04:11 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 14:04:11 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 14:05:54 ot_train.py:351 step:4K smpl:256K ep:811 epch:13.52 loss:0.019 grdn:0.275 lr:9.1e-05 updt_s:0.463 data_s:0.046\n",
      "WARNING 2025-12-13 14:05:54 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 14:05:54 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 14:07:35 ot_train.py:351 step:4K smpl:269K ep:852 epch:14.19 loss:0.018 grdn:0.273 lr:9.0e-05 updt_s:0.463 data_s:0.045\n",
      "WARNING 2025-12-13 14:07:35 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 14:07:35 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 14:46:01 ot_train.py:351 step:9K smpl:563K ep:2K epch:29.74 loss:0.010 grdn:0.195 lr:6.1e-05 updt_s:0.463 data_s:0.015\n",
      "WARNING 2025-12-13 14:46:01 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 14:46:01 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 14:47:44 ot_train.py:351 step:9K smpl:576K ep:2K epch:30.41 loss:0.010 grdn:0.188 lr:6.0e-05 updt_s:0.465 data_s:0.048\n",
      "WARNING 2025-12-13 14:47:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 14:47:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 14:49:27 ot_train.py:351 step:9K smpl:589K ep:2K epch:31.09 loss:0.009 grdn:0.193 lr:5.8e-05 updt_s:0.464 data_s:0.046\n",
      "WARNING 2025-12-13 14:49:27 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 14:49:27 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 14:51:03 ot_train.py:351 step:9K smpl:602K ep:2K epch:31.76 loss:0.009 grdn:0.185 lr:5.7e-05 updt_s:0.465 data_s:0.015\n",
      "WARNING 2025-12-13 14:51:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 14:51:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 14:52:45 ot_train.py:351 step:10K smpl:614K ep:2K epch:32.44 loss:0.009 grdn:0.175 lr:5.5e-05 updt_s:0.463 data_s:0.046\n",
      "WARNING 2025-12-13 14:52:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 14:52:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 14:54:28 ot_train.py:351 step:10K smpl:627K ep:2K epch:33.12 loss:0.009 grdn:0.179 lr:5.4e-05 updt_s:0.462 data_s:0.049\n",
      "WARNING 2025-12-13 14:54:28 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 14:54:28 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 14:56:04 ot_train.py:351 step:10K smpl:640K ep:2K epch:33.79 loss:0.008 grdn:0.178 lr:5.2e-05 updt_s:0.465 data_s:0.015\n",
      "WARNING 2025-12-13 14:56:04 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 14:56:04 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 14:56:04 ot_train.py:361 Checkpoint policy after step 10000\n",
      "INFO 2025-12-13 15:52:59 ot_train.py:351 step:17K smpl:1M ep:3K epch:56.77 loss:0.006 grdn:0.115 lr:8.9e-06 updt_s:0.467 data_s:0.015\n",
      "WARNING 2025-12-13 15:52:59 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 15:52:59 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 15:54:42 ot_train.py:351 step:17K smpl:1M ep:3K epch:57.44 loss:0.005 grdn:0.109 lr:8.2e-06 updt_s:0.468 data_s:0.042\n",
      "WARNING 2025-12-13 15:54:42 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 15:54:42 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 15:56:25 ot_train.py:351 step:17K smpl:1M ep:3K epch:58.12 loss:0.005 grdn:0.106 lr:7.5e-06 updt_s:0.469 data_s:0.044\n",
      "WARNING 2025-12-13 15:56:25 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 15:56:25 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 15:58:02 ot_train.py:351 step:17K smpl:1M ep:4K epch:58.80 loss:0.005 grdn:0.104 lr:6.8e-06 updt_s:0.470 data_s:0.016\n",
      "WARNING 2025-12-13 15:58:02 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 15:58:02 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 15:59:45 ot_train.py:351 step:18K smpl:1M ep:4K epch:59.47 loss:0.005 grdn:0.105 lr:6.2e-06 updt_s:0.468 data_s:0.044\n",
      "WARNING 2025-12-13 15:59:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 15:59:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 16:01:26 ot_train.py:351 step:18K smpl:1M ep:4K epch:60.15 loss:0.005 grdn:0.109 lr:5.6e-06 updt_s:0.463 data_s:0.044\n",
      "WARNING 2025-12-13 16:01:26 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 16:01:26 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 16:03:02 ot_train.py:351 step:18K smpl:1M ep:4K epch:60.82 loss:0.005 grdn:0.103 lr:5.1e-06 updt_s:0.464 data_s:0.015\n",
      "WARNING 2025-12-13 16:03:02 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 16:03:02 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 16:04:45 ot_train.py:351 step:18K smpl:1M ep:4K epch:61.50 loss:0.005 grdn:0.101 lr:4.7e-06 updt_s:0.464 data_s:0.045\n",
      "WARNING 2025-12-13 16:04:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 16:04:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 16:06:26 ot_train.py:351 step:18K smpl:1M ep:4K epch:62.18 loss:0.005 grdn:0.101 lr:4.2e-06 updt_s:0.465 data_s:0.042\n",
      "WARNING 2025-12-13 16:06:26 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 16:06:26 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 16:08:03 ot_train.py:351 step:19K smpl:1M ep:4K epch:62.85 loss:0.005 grdn:0.100 lr:3.8e-06 updt_s:0.467 data_s:0.015\n",
      "WARNING 2025-12-13 16:08:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 16:08:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 16:09:45 ot_train.py:351 step:19K smpl:1M ep:4K epch:63.53 loss:0.005 grdn:0.102 lr:3.5e-06 updt_s:0.464 data_s:0.046\n",
      "WARNING 2025-12-13 16:09:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 16:09:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 16:11:29 ot_train.py:351 step:19K smpl:1M ep:4K epch:64.20 loss:0.005 grdn:0.097 lr:3.2e-06 updt_s:0.467 data_s:0.050\n",
      "WARNING 2025-12-13 16:11:29 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 16:11:29 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 16:13:05 ot_train.py:351 step:19K smpl:1M ep:4K epch:64.88 loss:0.005 grdn:0.099 lr:3.0e-06 updt_s:0.467 data_s:0.015\n",
      "WARNING 2025-12-13 16:13:05 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 16:13:05 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 16:14:47 ot_train.py:351 step:19K smpl:1M ep:4K epch:65.55 loss:0.005 grdn:0.096 lr:2.8e-06 updt_s:0.462 data_s:0.046\n",
      "WARNING 2025-12-13 16:14:47 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 16:14:47 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 16:16:29 ot_train.py:351 step:20K smpl:1M ep:4K epch:66.23 loss:0.005 grdn:0.094 lr:2.7e-06 updt_s:0.463 data_s:0.047\n",
      "WARNING 2025-12-13 16:16:29 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 16:16:29 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 16:18:06 ot_train.py:351 step:20K smpl:1M ep:4K epch:66.91 loss:0.005 grdn:0.094 lr:2.6e-06 updt_s:0.468 data_s:0.015\n",
      "WARNING 2025-12-13 16:18:06 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 16:18:06 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-13 16:19:49 ot_train.py:351 step:20K smpl:1M ep:4K epch:67.58 loss:0.005 grdn:0.096 lr:2.5e-06 updt_s:0.463 data_s:0.051\n",
      "WARNING 2025-12-13 16:19:49 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-13 16:19:49 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-13 16:19:49 ot_train.py:361 Checkpoint policy after step 20000\n",
      "INFO 2025-12-13 16:19:53 ot_train.py:430 End of training\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 407, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/venv/lib/python3.12/site-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/venv/bin/lerobot-train\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/workspace/lerobot/src/lerobot/scripts/lerobot_train.py\", line 444, in main\n",
      "    train()\n",
      "  File \"/workspace/lerobot/src/lerobot/configs/parser.py\", line 233, in wrapper_inner\n",
      "    response = fn(cfg, *args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/lerobot/src/lerobot/scripts/lerobot_train.py\", line 434, in train\n",
      "    unwrapped_policy.push_model_to_hub(cfg)\n",
      "  File \"/workspace/lerobot/src/lerobot/policies/pretrained.py\", line 211, in push_model_to_hub\n",
      "    repo_id = api.create_repo(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py\", line 3779, in create_repo\n",
      "    raise err\n",
      "  File \"/opt/venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py\", line 3766, in create_repo\n",
      "    hf_raise_for_status(r)\n",
      "  File \"/opt/venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 471, in hf_raise_for_status\n",
      "    raise _format(HfHubHTTPError, message, response) from e\n",
      "huggingface_hub.errors.HfHubHTTPError: (Request ID: Root=1-693d9229-4643934f69eceb376e58c0fd;65217c0a-f7dd-49f3-b032-3c6bd67a61d7)\n",
      "\n",
      "403 Forbidden: You don't have the rights to create a model under the namespace \"giacomoran\".\n",
      "Cannot access content at: https://huggingface.co/api/repos/create.\n",
      "Make sure your token has the correct permissions.\n"
     ]
    }
   ],
   "source": [
    "!lerobot-train \\\n",
    "  --dataset.repo_id=giacomoran/hackathon_amd_mission2_black_sort \\\n",
    "  --rename_map='{\"observation.images.top\": \"observation.images.camera1\", \"observation.images.wrist\": \"observation.images.camera2\"}' \\\n",
    "  --policy.empty_cameras=1 \\\n",
    "  --batch_size=64 \\\n",
    "  --steps=20000 \\\n",
    "  --output_dir=outputs/train/hackathon_amd_mission2_black_sort_smolvla \\\n",
    "  --job_name=hackathon_amd_mission2_black_sort_smolvla \\\n",
    "  --policy.repo_id=giacomoran/hackathon_amd_mission2_black_sort_smolvla \\\n",
    "  --policy.device=cuda \\\n",
    "  --policy.path=lerobot/smolvla_base \\\n",
    "  --policy.push_to_hub=true \\\n",
    "  --wandb.enable=true \\\n",
    "  --policy.chunk_size=30 \\\n",
    "  --policy.n_action_steps=30 \\\n",
    "  --save_freq=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\n",
      "model.safetensors\n",
      "policy_postprocessor.json\n",
      "policy_postprocessor_step_0_unnormalizer_processor.safetensors\n",
      "policy_preprocessor.json\n",
      "policy_preprocessor_step_5_normalizer_processor.safetensors\n",
      "train_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls outputs/train/hackathon_amd_mission2_black_sort_smolvla/checkpoints/last/pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m‚ö†Ô∏è  Warning: 'huggingface-cli upload' is deprecated. Use 'hf upload' instead.\u001b[0m\n",
      "Start hashing 7 files.\n",
      "Finished hashing 7 files.\n",
      "Processing Files (0 / 0)      : |                  |  0.00B /  0.00B            \n",
      "New Data Upload               : |                  |  0.00B /  0.00B            \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (2 / 2)      :   0%|              | 15.1kB /  907MB,   ???B/s  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  ...d_model/model.safetensors:   1%|‚ñè             | 9.13MB /  907MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :   1%|‚ñè             | 9.15MB /  907MB, 22.9MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               :   7%|‚ñâ             | 9.13MB /  134MB, 22.9MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :   4%|‚ñå             | 39.0MB /  907MB, 65.0MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               :  29%|‚ñà‚ñà‚ñà‚ñà          | 39.0MB /  134MB, 65.0MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  11%|‚ñà‚ñå            |  101MB /  907MB,  126MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               :  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       |  101MB /  201MB,  126MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  15%|‚ñà‚ñà‚ñè           |  139MB /  907MB,  139MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               :  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    |  139MB /  201MB,  139MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  22%|‚ñà‚ñà‚ñà           |  199MB /  907MB,  165MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               :  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  |  165MB /  201MB,  138MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        |  352MB /  907MB,  251MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ|  201MB /  201MB,  143MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      |  469MB /  907MB,  293MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ|  201MB /  201MB,  125MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    |  595MB /  907MB,  331MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ|  201MB /  201MB,  112MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   |  712MB /  907MB,  356MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä |  830MB /  907MB,  377MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  201MB /  201MB, 91.4MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå|  877MB /  907MB,  366MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               :  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè |  205MB /  235MB, 85.5MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ|  902MB /  907MB,  347MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               :  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã|  230MB /  235MB, 88.3MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ|  906MB /  907MB,  324MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ|  234MB /  235MB, 83.7MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  ...d_model/model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ|  906MB /  907MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (3 / 3)      : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  907MB /  907MB,  283MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  235MB /  235MB, 73.4MB/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  ...d_model/model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  907MB /  907MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  ...d_model/model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  907MB /  907MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (3 / 3)      : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  907MB /  907MB,  252MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  235MB /  235MB, 65.2MB/s  \n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \n",
      "  ...zer_processor.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.54kB / 7.54kB            \n",
      "  ...d_model/model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  907MB /  907MB            \n",
      "https://huggingface.co/giacomoran/hackathon_amd_mission2_black_sort_smolvla/tree/main/.\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli upload giacomoran/hackathon_amd_mission2_black_sort_smolvla \\\n",
    "  outputs/train/hackathon_amd_mission2_black_sort_smolvla/checkpoints/last/pretrained_model \\\n",
    "  ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "- If using a local dataset, add `--dataset.root=/path/to/dataset`.\n",
    "- Adjust `--batch_size` and `--steps` based on your hardware and dataset.\n",
    "- Model checkpoints, logs, and training plots will be saved to the specified `--output_dir`\n",
    "- Training progress visualized in your wandb dashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Models from Hugging Face to Local Machine\n",
    "\n",
    "Now after training is done, download the model to local machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFMLGuVkH7UN",
    "outputId": "535f0717-4d6b-4eeb-beee-25416f7af383",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m‚ö†Ô∏è  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Fetching 9 files:   0%|                                   | 0/9 [00:00<?, ?it/s]Downloading 'policy_preprocessor_step_3_normalizer_processor.safetensors' to 'outputs/train/hackathon_amd_mission1/.cache/huggingface/download/KTJX-ilfk9YZ9PyHew1aNVsbRpk=.75bebdcbfc0f385e5ac9989dbe6cc1d61b17283609bb5ad4a05911a6e6ad4727.incomplete'\n",
      "Downloading 'policy_postprocessor_step_0_unnormalizer_processor.safetensors' to 'outputs/train/hackathon_amd_mission1/.cache/huggingface/download/v3ZsRgUbr2uP9oX0M65G05JpmcY=.75bebdcbfc0f385e5ac9989dbe6cc1d61b17283609bb5ad4a05911a6e6ad4727.incomplete'\n",
      "Downloading 'model.safetensors' to 'outputs/train/hackathon_amd_mission1/.cache/huggingface/download/xGOKKLRSlIhH692hSVvI1-gpoa8=.e3f98e2557158f8e671fab106f96cf0b06b37b3a2e326e52fab9894c67352553.incomplete'\n",
      "Downloading 'config.json' to 'outputs/train/hackathon_amd_mission1/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.15d2585d88947322af01e0e632e30b31f9cfd91d.incomplete'\n",
      "Downloading 'policy_postprocessor.json' to 'outputs/train/hackathon_amd_mission1/.cache/huggingface/download/X2g7UNq7DXuEoJRqAditBg4Y6Kg=.9ccb342474cecbf2bdbb034d27169a351ad96d2a.incomplete'\n",
      "Downloading 'policy_preprocessor.json' to 'outputs/train/hackathon_amd_mission1/.cache/huggingface/download/nLTUUgZhacDJIiwj_-WDKwdZVT0=.508543eca8c25d232618ff21b0e28a7570a5fd1c.incomplete'\n",
      "\n",
      "policy_preprocessor_step_3_normalizer_pr(‚Ä¶):   0%|  | 0.00/7.54k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "config.json: 1.67kB [00:00, 12.3MB/s]A\n",
      "Download complete. Moving file to outputs/train/hackathon_amd_mission1/config.json\n",
      "\n",
      "\n",
      "policy_postprocessor.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 660/660 [00:00<00:00, 9.26MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to outputs/train/hackathon_amd_mission1/policy_postprocessor.json\n",
      "Downloading '.gitattributes' to 'outputs/train/hackathon_amd_mission1/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
      "Downloading 'README.md' to 'outputs/train/hackathon_amd_mission1/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.f2c3b3709119efb4cb2081faf5ae2ea7cbb245bc.incomplete'\n",
      "\n",
      "\n",
      "policy_preprocessor.json: 1.33kB [00:00, 3.77MB/s]A\n",
      "Download complete. Moving file to outputs/train/hackathon_amd_mission1/policy_preprocessor.json\n",
      "\n",
      "\n",
      "policy_postprocessor_step_0_unnormalizer(‚Ä¶):   0%|  | 0.00/7.54k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   0%|                             | 0.00/206M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      ".gitattributes: 1.52kB [00:00, 16.7MB/s]A\u001b[A\u001b[A\n",
      "Download complete. Moving file to outputs/train/hackathon_amd_mission1/.gitattributes\n",
      "Fetching 9 files:  11%|‚ñà‚ñà‚ñà                        | 1/9 [00:00<00:02,  3.09it/s]\n",
      "\n",
      "\n",
      "\n",
      "README.md: 1.69kB [00:00, 15.1MB/s]A\u001b[A\u001b[A\n",
      "Download complete. Moving file to outputs/train/hackathon_amd_mission1/README.md\n",
      "Downloading 'train_config.json' to 'outputs/train/hackathon_amd_mission1/.cache/huggingface/download/h0POgHcaMMWzN_7b04nXzkOh13M=.c16760f6ce476bac8a60b6836de3f61e96cd8525.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train_config.json: 5.36kB [00:00, 37.5MB/s]A\u001b[A\u001b[A\n",
      "Download complete. Moving file to outputs/train/hackathon_amd_mission1/train_config.json\n",
      "\n",
      "policy_preprocessor_step_3_normalizer_pr(‚Ä¶): 100%|‚ñà| 7.54k/7.54k [00:00<00:00, 3\u001b[A\n",
      "Download complete. Moving file to outputs/train/hackathon_amd_mission1/policy_preprocessor_step_3_normalizer_processor.safetensors\n",
      "\n",
      "\n",
      "policy_postprocessor_step_0_unnormalizer(‚Ä¶): 100%|‚ñà| 7.54k/7.54k [00:00<00:00, 3\u001b[A\u001b[A\n",
      "Download complete. Moving file to outputs/train/hackathon_amd_mission1/policy_postprocessor_step_0_unnormalizer_processor.safetensors\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   2%|‚ñç                   | 4.65M/206M [00:00<00:23, 8.47MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 206M/206M [00:01<00:00, 177MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to outputs/train/hackathon_amd_mission1/model.safetensors\n",
      "Fetching 9 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:01<00:00,  6.10it/s]\n",
      "/workspace/outputs/train/hackathon_amd_mission1\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download giacomoran/hackathon_amd_mission1 --repo-type model --local-dir outputs/train/hackathon_amd_mission1\n",
    "# e.g. huggingface-cli upload ${HF_USER}/act_so101_3cube_1ksteps \\\n",
    "#  outputs/train/act_so101_3cube_1ksteps/checkpoints/last/pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install -y zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: outputs/train/hackathon_amd_mission2_smallvla/wandb: Cannot stat: No such file or directory\n",
      "tar: Exiting with failure status due to previous errors\n"
     ]
    }
   ],
   "source": [
    "!tar -czvf wandb.tar.gz outputs/train/hackathon_amd_mission2_smallvla/wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscs\n",
    "\n",
    "1. Once the environment is setup, you can open a terminal session for training by navigating to `File ‚Üí New Launcher ‚Üí Other ‚Üí Terminal`.\n",
    "2. You can also upload your datasets to the container by clicking the `Upload Files` button in the left pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "\n",
    "1. If you encounter an error like:\n",
    "   ```\n",
    "   FileExistsError: Output directory outputs/train/act_so101_3cube_1ksteps already exists and resume is False. Please change your output directory so that outputs/train/act_so101_3cube_1ksteps is not overwritten.\n",
    "   ```\n",
    "   Remove the existing directory before proceeding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFMLGuVkH7UN",
    "outputId": "535f0717-4d6b-4eeb-beee-25416f7af383"
   },
   "outputs": [],
   "source": [
    "!rm -fr outputs/train/hackathon_amd_mission1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. When running models other than ACT, ensure you install the required additional dependencies for those models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For smolVLA\n",
    "!cd lerobot && pip install -e \".[smolvla]\"\n",
    "# For Pi\n",
    "!cd lerobot && pip install -e \".[pi]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If you want to resume the training from last checkpoint, run the command below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: lerobot-train [-h] [--config_path str] [--dataset str]\n",
      "                     [--dataset.repo_id str] [--dataset.root str]\n",
      "                     [--dataset.episodes str] [--image_transforms str]\n",
      "                     [--dataset.image_transforms.enable str]\n",
      "                     [--dataset.image_transforms.max_num_transforms str]\n",
      "                     [--dataset.image_transforms.random_order str]\n",
      "                     [--dataset.image_transforms.tfs str]\n",
      "                     [--dataset.revision str]\n",
      "                     [--dataset.use_imagenet_stats str]\n",
      "                     [--dataset.video_backend str] [--dataset.streaming str]\n",
      "                     [--env str]\n",
      "                     [--env.type {aloha,pusht,gym_manipulator,libero,metaworld}]\n",
      "                     [--env.visualization_width str]\n",
      "                     [--env.visualization_height str] [--robot str]\n",
      "                     [--env.robot.type str] [--teleop str]\n",
      "                     [--env.teleop.type str] [--processor str]\n",
      "                     [--env.processor.control_mode str] [--observation str]\n",
      "                     [--env.processor.observation.add_joint_velocity_to_observation str]\n",
      "                     [--env.processor.observation.add_current_to_observation str]\n",
      "                     [--env.processor.observation.display_cameras str]\n",
      "                     [--image_preprocessing str]\n",
      "                     [--env.processor.image_preprocessing.crop_params_dict str]\n",
      "                     [--env.processor.image_preprocessing.resize_size str]\n",
      "                     [--gripper str] [--env.processor.gripper.use_gripper str]\n",
      "                     [--env.processor.gripper.gripper_penalty str]\n",
      "                     [--reset str]\n",
      "                     [--env.processor.reset.fixed_reset_joint_positions str]\n",
      "                     [--env.processor.reset.reset_time_s str]\n",
      "                     [--env.processor.reset.control_time_s str]\n",
      "                     [--env.processor.reset.terminate_on_success str]\n",
      "                     [--inverse_kinematics str]\n",
      "                     [--env.processor.inverse_kinematics.urdf_path str]\n",
      "                     [--env.processor.inverse_kinematics.target_frame_name str]\n",
      "                     [--env.processor.inverse_kinematics.end_effector_bounds str]\n",
      "                     [--env.processor.inverse_kinematics.end_effector_step_sizes str]\n",
      "                     [--reward_classifier str]\n",
      "                     [--env.processor.reward_classifier.pretrained_path str]\n",
      "                     [--env.processor.reward_classifier.success_threshold str]\n",
      "                     [--env.processor.reward_classifier.success_reward str]\n",
      "                     [--env.processor.max_gripper_pos str] [--env.name str]\n",
      "                     [--env.camera_name str] [--env.init_states str]\n",
      "                     [--env.camera_name_mapping str]\n",
      "                     [--env.observation_height str]\n",
      "                     [--env.observation_width str] [--env.task str]\n",
      "                     [--env.fps str] [--env.features str]\n",
      "                     [--env.features_map str] [--env.max_parallel_tasks str]\n",
      "                     [--env.disable_env_checker str]\n",
      "                     [--env.episode_length str] [--env.obs_type str]\n",
      "                     [--env.render_mode str] [--env.multitask_eval str]\n",
      "                     [--policy str]\n",
      "                     [--policy.type {act,diffusion,groot,pi0,pi05,smolvla,tdmpc,vqbet,sac,reward_classifier}]\n",
      "                     [--policy.replace_final_stride_with_dilation str]\n",
      "                     [--policy.pre_norm str] [--policy.dim_model str]\n",
      "                     [--policy.n_heads str] [--policy.dim_feedforward str]\n",
      "                     [--policy.feedforward_activation str]\n",
      "                     [--policy.n_encoder_layers str]\n",
      "                     [--policy.n_decoder_layers str] [--policy.use_vae str]\n",
      "                     [--policy.n_vae_encoder_layers str]\n",
      "                     [--policy.temporal_ensemble_coeff str]\n",
      "                     [--policy.kl_weight str]\n",
      "                     [--policy.optimizer_lr_backbone str]\n",
      "                     [--policy.drop_n_last_frames str]\n",
      "                     [--policy.use_separate_rgb_encoder_per_camera str]\n",
      "                     [--policy.down_dims str] [--policy.kernel_size str]\n",
      "                     [--policy.n_groups str]\n",
      "                     [--policy.diffusion_step_embed_dim str]\n",
      "                     [--policy.use_film_scale_modulation str]\n",
      "                     [--policy.noise_scheduler_type str]\n",
      "                     [--policy.num_train_timesteps str]\n",
      "                     [--policy.beta_schedule str] [--policy.beta_start str]\n",
      "                     [--policy.beta_end str] [--policy.prediction_type str]\n",
      "                     [--policy.clip_sample str]\n",
      "                     [--policy.clip_sample_range str]\n",
      "                     [--policy.do_mask_loss_for_padding str]\n",
      "                     [--policy.scheduler_name str] [--policy.image_size str]\n",
      "                     [--policy.base_model_path str]\n",
      "                     [--policy.tokenizer_assets_repo str]\n",
      "                     [--policy.embodiment_tag str] [--policy.tune_llm str]\n",
      "                     [--policy.tune_visual str] [--policy.tune_projector str]\n",
      "                     [--policy.tune_diffusion_model str]\n",
      "                     [--policy.lora_rank str] [--policy.lora_alpha str]\n",
      "                     [--policy.lora_dropout str]\n",
      "                     [--policy.lora_full_model str]\n",
      "                     [--policy.warmup_ratio str] [--policy.use_bf16 str]\n",
      "                     [--policy.video_backend str]\n",
      "                     [--policy.balance_dataset_weights str]\n",
      "                     [--policy.balance_trajectory_weights str]\n",
      "                     [--policy.dataset_paths str] [--policy.output_dir str]\n",
      "                     [--policy.save_steps str] [--policy.max_steps str]\n",
      "                     [--policy.batch_size str]\n",
      "                     [--policy.dataloader_num_workers str]\n",
      "                     [--policy.report_to str] [--policy.resume str]\n",
      "                     [--policy.paligemma_variant str]\n",
      "                     [--policy.action_expert_variant str] [--policy.dtype str]\n",
      "                     [--policy.num_inference_steps str]\n",
      "                     [--policy.time_sampling_beta_alpha str]\n",
      "                     [--policy.time_sampling_beta_beta str]\n",
      "                     [--policy.time_sampling_scale str]\n",
      "                     [--policy.time_sampling_offset str]\n",
      "                     [--policy.image_resolution str]\n",
      "                     [--policy.gradient_checkpointing str]\n",
      "                     [--policy.compile_model str] [--policy.compile_mode str]\n",
      "                     [--policy.chunk_size str] [--policy.max_state_dim str]\n",
      "                     [--policy.max_action_dim str]\n",
      "                     [--policy.resize_imgs_with_padding str]\n",
      "                     [--policy.empty_cameras str]\n",
      "                     [--policy.adapt_to_pi_aloha str]\n",
      "                     [--policy.use_delta_joint_actions_aloha str]\n",
      "                     [--policy.tokenizer_max_length str]\n",
      "                     [--policy.num_steps str] [--policy.use_cache str]\n",
      "                     [--policy.train_expert_only str]\n",
      "                     [--policy.train_state_proj str]\n",
      "                     [--policy.optimizer_grad_clip_norm str]\n",
      "                     [--policy.scheduler_decay_steps str]\n",
      "                     [--policy.scheduler_decay_lr str]\n",
      "                     [--policy.vlm_model_name str]\n",
      "                     [--policy.load_vlm_weights str]\n",
      "                     [--policy.add_image_special_tokens str]\n",
      "                     [--policy.attention_mode str]\n",
      "                     [--policy.prefix_length str]\n",
      "                     [--policy.pad_language_to str]\n",
      "                     [--policy.num_expert_layers str]\n",
      "                     [--policy.num_vlm_layers str]\n",
      "                     [--policy.self_attn_every_n_layers str]\n",
      "                     [--policy.expert_width_multiplier str]\n",
      "                     [--policy.min_period str] [--policy.max_period str]\n",
      "                     [--policy.n_action_repeats str] [--policy.horizon str]\n",
      "                     [--policy.n_action_steps str]\n",
      "                     [--policy.q_ensemble_size str] [--policy.mlp_dim str]\n",
      "                     [--policy.use_mpc str] [--policy.cem_iterations str]\n",
      "                     [--policy.max_std str] [--policy.min_std str]\n",
      "                     [--policy.n_gaussian_samples str]\n",
      "                     [--policy.n_pi_samples str]\n",
      "                     [--policy.uncertainty_regularizer_coeff str]\n",
      "                     [--policy.n_elites str]\n",
      "                     [--policy.elite_weighting_temperature str]\n",
      "                     [--policy.gaussian_mean_momentum str]\n",
      "                     [--policy.max_random_shift_ratio str]\n",
      "                     [--policy.reward_coeff str]\n",
      "                     [--policy.expectile_weight str]\n",
      "                     [--policy.value_coeff str]\n",
      "                     [--policy.consistency_coeff str]\n",
      "                     [--policy.advantage_scaling str] [--policy.pi_coeff str]\n",
      "                     [--policy.temporal_decay_coeff str]\n",
      "                     [--policy.target_model_momentum str]\n",
      "                     [--policy.n_action_pred_token str]\n",
      "                     [--policy.action_chunk_size str]\n",
      "                     [--policy.vision_backbone str] [--policy.crop_shape str]\n",
      "                     [--policy.crop_is_random str]\n",
      "                     [--policy.pretrained_backbone_weights str]\n",
      "                     [--policy.use_group_norm str]\n",
      "                     [--policy.spatial_softmax_num_keypoints str]\n",
      "                     [--policy.n_vqvae_training_steps str]\n",
      "                     [--policy.vqvae_n_embed str]\n",
      "                     [--policy.vqvae_embedding_dim str]\n",
      "                     [--policy.vqvae_enc_hidden_dim str]\n",
      "                     [--policy.gpt_block_size str]\n",
      "                     [--policy.gpt_input_dim str]\n",
      "                     [--policy.gpt_output_dim str] [--policy.gpt_n_layer str]\n",
      "                     [--policy.gpt_n_head str] [--policy.gpt_hidden_dim str]\n",
      "                     [--policy.dropout str] [--policy.offset_loss_weight str]\n",
      "                     [--policy.primary_code_loss_weight str]\n",
      "                     [--policy.secondary_code_loss_weight str]\n",
      "                     [--policy.bet_softmax_temperature str]\n",
      "                     [--policy.sequentially_select str]\n",
      "                     [--policy.optimizer_lr str]\n",
      "                     [--policy.optimizer_betas str]\n",
      "                     [--policy.optimizer_eps str]\n",
      "                     [--policy.optimizer_weight_decay str]\n",
      "                     [--policy.optimizer_vqvae_lr str]\n",
      "                     [--policy.optimizer_vqvae_weight_decay str]\n",
      "                     [--policy.scheduler_warmup_steps str]\n",
      "                     [--policy.dataset_stats str]\n",
      "                     [--policy.storage_device str]\n",
      "                     [--policy.vision_encoder_name str]\n",
      "                     [--policy.freeze_vision_encoder str]\n",
      "                     [--policy.image_encoder_hidden_dim str]\n",
      "                     [--policy.shared_encoder str]\n",
      "                     [--policy.num_discrete_actions str]\n",
      "                     [--policy.online_steps str]\n",
      "                     [--policy.online_buffer_capacity str]\n",
      "                     [--policy.offline_buffer_capacity str]\n",
      "                     [--policy.async_prefetch str]\n",
      "                     [--policy.online_step_before_learning str]\n",
      "                     [--policy.policy_update_freq str] [--policy.discount str]\n",
      "                     [--policy.temperature_init str]\n",
      "                     [--policy.num_critics str]\n",
      "                     [--policy.num_subsample_critics str]\n",
      "                     [--policy.critic_lr str] [--policy.actor_lr str]\n",
      "                     [--policy.temperature_lr str]\n",
      "                     [--policy.critic_target_update_weight str]\n",
      "                     [--policy.utd_ratio str]\n",
      "                     [--policy.state_encoder_hidden_dim str]\n",
      "                     [--policy.target_entropy str]\n",
      "                     [--policy.use_backup_entropy str]\n",
      "                     [--critic_network_kwargs str]\n",
      "                     [--policy.critic_network_kwargs.hidden_dims str]\n",
      "                     [--policy.critic_network_kwargs.activate_final str]\n",
      "                     [--policy.critic_network_kwargs.final_activation str]\n",
      "                     [--actor_network_kwargs str]\n",
      "                     [--policy.actor_network_kwargs.hidden_dims str]\n",
      "                     [--policy.actor_network_kwargs.activate_final str]\n",
      "                     [--policy_kwargs str]\n",
      "                     [--policy.policy_kwargs.use_tanh_squash str]\n",
      "                     [--policy.policy_kwargs.std_min str]\n",
      "                     [--policy.policy_kwargs.std_max str]\n",
      "                     [--policy.policy_kwargs.init_final str]\n",
      "                     [--discrete_critic_network_kwargs str]\n",
      "                     [--policy.discrete_critic_network_kwargs.hidden_dims str]\n",
      "                     [--policy.discrete_critic_network_kwargs.activate_final str]\n",
      "                     [--policy.discrete_critic_network_kwargs.final_activation str]\n",
      "                     [--actor_learner_config str]\n",
      "                     [--policy.actor_learner_config.learner_host str]\n",
      "                     [--policy.actor_learner_config.learner_port str]\n",
      "                     [--policy.actor_learner_config.policy_parameters_push_frequency str]\n",
      "                     [--policy.actor_learner_config.queue_get_timeout str]\n",
      "                     [--concurrency str] [--policy.concurrency.actor str]\n",
      "                     [--policy.concurrency.learner str]\n",
      "                     [--policy.use_torch_compile str]\n",
      "                     [--policy.n_obs_steps str] [--policy.input_features str]\n",
      "                     [--policy.output_features str] [--policy.device str]\n",
      "                     [--policy.use_amp str] [--policy.push_to_hub str]\n",
      "                     [--policy.repo_id str] [--policy.private str]\n",
      "                     [--policy.tags str] [--policy.license str]\n",
      "                     [--policy.pretrained_path str] [--policy.name str]\n",
      "                     [--policy.num_classes str] [--policy.hidden_dim str]\n",
      "                     [--policy.latent_dim str]\n",
      "                     [--policy.image_embedding_pooling_dim str]\n",
      "                     [--policy.dropout_rate str] [--policy.model_name str]\n",
      "                     [--policy.model_type str] [--policy.num_cameras str]\n",
      "                     [--policy.learning_rate str] [--policy.weight_decay str]\n",
      "                     [--policy.grad_clip_norm str]\n",
      "                     [--policy.normalization_mapping str] [--output_dir str]\n",
      "                     [--job_name str] [--resume str] [--seed str]\n",
      "                     [--num_workers str] [--batch_size str] [--steps str]\n",
      "                     [--eval_freq str] [--log_freq str]\n",
      "                     [--save_checkpoint str] [--save_freq str]\n",
      "                     [--use_policy_training_preset str] [--optimizer str]\n",
      "                     [--optimizer.type {adam,adamw,sgd,multi_adam}]\n",
      "                     [--optimizer.betas str] [--optimizer.eps str]\n",
      "                     [--optimizer.momentum str] [--optimizer.dampening str]\n",
      "                     [--optimizer.nesterov str] [--optimizer.lr str]\n",
      "                     [--optimizer.weight_decay str]\n",
      "                     [--optimizer.grad_clip_norm str]\n",
      "                     [--optimizer.optimizer_groups str] [--scheduler str]\n",
      "                     [--scheduler.type {diffuser,vqbet,cosine_decay_with_warmup}]\n",
      "                     [--scheduler.name str]\n",
      "                     [--scheduler.num_vqvae_training_steps str]\n",
      "                     [--scheduler.num_cycles str]\n",
      "                     [--scheduler.num_warmup_steps str]\n",
      "                     [--scheduler.num_decay_steps str]\n",
      "                     [--scheduler.peak_lr str] [--scheduler.decay_lr str]\n",
      "                     [--eval str] [--eval.n_episodes str]\n",
      "                     [--eval.batch_size str] [--eval.use_async_envs str]\n",
      "                     [--wandb str] [--wandb.enable str]\n",
      "                     [--wandb.disable_artifact str] [--wandb.project str]\n",
      "                     [--wandb.entity str] [--wandb.notes str]\n",
      "                     [--wandb.run_id str] [--wandb.mode str]\n",
      "                     [--rename_map str]\n",
      "lerobot-train: error: unrecognized arguments: \\\n"
     ]
    }
   ],
   "source": [
    "!lerobot-train \\\n",
    "  --resume=true \\\n",
    "  --config_path=outputs/train/hackathon_amd_mission2_black_sort/checkpoints/last/pretrained_model/train_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. If you want to upload your dataset using `huggingface-cli upload <repo name> <path to the dataset> --repo-type=dataset`, be sure to set a codebase tag like below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"your_huggingface_token\")\n",
    "hub_api = HfApi()\n",
    "hub_api.create_tag(<HF_REPO_NAME>, tag=\"v3.0\", revision=\"main\", repo_type=\"dataset\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
